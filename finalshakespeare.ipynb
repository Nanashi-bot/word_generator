{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z8jtYD1dioI6"
   },
   "outputs": [],
   "source": [
    "# Load Shakespeare's text\n",
    "with open('shake.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "HKFfiuh5jwn5",
    "outputId": "aa01c598-e863-4931-ed25-bc066d2021b7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MXzlk7pDjtwO"
   },
   "outputs": [],
   "source": [
    "text = text.lower().replace('\\n', ' ').replace('$', '').replace('&', '').replace('3', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "6jCzu5QQj7zA",
    "outputId": "6bd0fc86-937a-4ab7-cac5-f452778f89a0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"first citizen: before we proceed any further, hear me speak.  all: speak, speak.  first citizen: you are all resolved rather to die than to famish?  all: resolved. resolved.  first citizen: first, you know caius marcius is chief enemy to the people.  all: we know't, we know't.  first citizen: let us kill him, and we'll have corn at our own price. is't a verdict?  all: no more talking on't; let it be done: away, away!  second citizen: one word, good citizens.  first citizen: we are accounted poor citizens, the patricians good. what authority surfeits on would relieve us: if they would yield us but the superfluity, while it were wholesome, we might guess they relieved us humanely; but they think we are too dear: the leanness that afflicts us, the object of our misery, is as an inventory to particularise their abundance; our sufferance is a gain to them let us revenge this with our pikes, ere we become rakes: for the gods know i speak this in hunger for bread, not in thirst for revenge.  \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyhkW4m6lp6S",
    "outputId": "2b167382-b882-4177-a6e4-375516161e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X1rUCDnYtjdV"
   },
   "outputs": [],
   "source": [
    "char_to_index = {char: idx for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ilfQY2WCHuDu"
   },
   "outputs": [],
   "source": [
    "index_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArTTPXtdtvHL",
    "outputId": "c6825656-a94e-43fb-cd2d-970b06a0062e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " \"'\": 2,\n",
       " ',': 3,\n",
       " '-': 4,\n",
       " '.': 5,\n",
       " ':': 6,\n",
       " ';': 7,\n",
       " '?': 8,\n",
       " 'a': 9,\n",
       " 'b': 10,\n",
       " 'c': 11,\n",
       " 'd': 12,\n",
       " 'e': 13,\n",
       " 'f': 14,\n",
       " 'g': 15,\n",
       " 'h': 16,\n",
       " 'i': 17,\n",
       " 'j': 18,\n",
       " 'k': 19,\n",
       " 'l': 20,\n",
       " 'm': 21,\n",
       " 'n': 22,\n",
       " 'o': 23,\n",
       " 'p': 24,\n",
       " 'q': 25,\n",
       " 'r': 26,\n",
       " 's': 27,\n",
       " 't': 28,\n",
       " 'u': 29,\n",
       " 'v': 30,\n",
       " 'w': 31,\n",
       " 'x': 32,\n",
       " 'y': 33,\n",
       " 'z': 34}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115363"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pErC_sZ159wO"
   },
   "outputs": [],
   "source": [
    "text = text[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XnhHebUmrer_"
   },
   "outputs": [],
   "source": [
    "vectorised_text = []\n",
    "for i in text:\n",
    "    vec = [0] * 35\n",
    "    vec[char_to_index[i]] = 1\n",
    "    vectorised_text.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "90HNcGuktgpN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vectorised_text = np.array(vectorised_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZkUcbhCuPdh",
    "outputId": "9c5d353c-5b5a-4e48-f903-f747a0cc46d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorised_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0cpsQfRwumyM"
   },
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(len(vectorised_text)-sequence_length):\n",
    "    sequences.append(vectorised_text[i:i+sequence_length])\n",
    "    targets.append(vectorised_text[i+sequence_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "HsebSBGdwCkl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_np = np.array(sequences)\n",
    "sequences_tensor = torch.tensor(sequences_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "C8SPMlN4wDTB"
   },
   "outputs": [],
   "source": [
    "targets_tensor = torch.tensor(targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WL9o_t_wwMYF"
   },
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # out: (batch_size, seq_length, hidden_size)\n",
    "        out = out[:, -1, :]  # Get the last time step\n",
    "        out = self.fc(out)  # Fully connected layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1dFrXMUWwRL9"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 35  # Size of one-hot encoding\n",
    "hidden_size = 128  # Number of LSTM units\n",
    "output_size = 35  # Same as input size for one-hot encoding\n",
    "num_epochs = 250\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogits for one-hot targets\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99980, 20, 35])\n",
      "torch.Size([99980, 35])\n"
     ]
    }
   ],
   "source": [
    "print(sequences_tensor.shape)\n",
    "print(targets_tensor.shape)\n",
    "\n",
    "sequences_tensor = sequences_tensor.to('cuda')\n",
    "targets_tensor = targets_tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Loss: 0.1139\n",
      "Epoch [2/250], Loss: 0.0931\n",
      "Epoch [3/250], Loss: 0.0879\n",
      "Epoch [4/250], Loss: 0.0850\n",
      "Epoch [5/250], Loss: 0.0827\n",
      "Epoch [6/250], Loss: 0.0805\n",
      "Epoch [7/250], Loss: 0.0786\n",
      "Epoch [8/250], Loss: 0.0769\n",
      "Epoch [9/250], Loss: 0.0754\n",
      "Epoch [10/250], Loss: 0.0741\n",
      "Epoch [11/250], Loss: 0.0729\n",
      "Epoch [12/250], Loss: 0.0719\n",
      "Epoch [13/250], Loss: 0.0709\n",
      "Epoch [14/250], Loss: 0.0701\n",
      "Epoch [15/250], Loss: 0.0693\n",
      "Epoch [16/250], Loss: 0.0686\n",
      "Epoch [17/250], Loss: 0.0679\n",
      "Epoch [18/250], Loss: 0.0673\n",
      "Epoch [19/250], Loss: 0.0668\n",
      "Epoch [20/250], Loss: 0.0663\n",
      "Epoch [21/250], Loss: 0.0658\n",
      "Epoch [22/250], Loss: 0.0653\n",
      "Epoch [23/250], Loss: 0.0649\n",
      "Epoch [24/250], Loss: 0.0645\n",
      "Epoch [25/250], Loss: 0.0642\n",
      "Epoch [26/250], Loss: 0.0638\n",
      "Epoch [27/250], Loss: 0.0635\n",
      "Epoch [28/250], Loss: 0.0631\n",
      "Epoch [29/250], Loss: 0.0628\n",
      "Epoch [30/250], Loss: 0.0625\n",
      "Epoch [31/250], Loss: 0.0623\n",
      "Epoch [32/250], Loss: 0.0620\n",
      "Epoch [33/250], Loss: 0.0617\n",
      "Epoch [34/250], Loss: 0.0615\n",
      "Epoch [35/250], Loss: 0.0612\n",
      "Epoch [36/250], Loss: 0.0610\n",
      "Epoch [37/250], Loss: 0.0607\n",
      "Epoch [38/250], Loss: 0.0605\n",
      "Epoch [39/250], Loss: 0.0602\n",
      "Epoch [40/250], Loss: 0.0600\n",
      "Epoch [41/250], Loss: 0.0598\n",
      "Epoch [42/250], Loss: 0.0596\n",
      "Epoch [43/250], Loss: 0.0593\n",
      "Epoch [44/250], Loss: 0.0591\n",
      "Epoch [45/250], Loss: 0.0588\n",
      "Epoch [46/250], Loss: 0.0587\n",
      "Epoch [47/250], Loss: 0.0584\n",
      "Epoch [48/250], Loss: 0.0582\n",
      "Epoch [49/250], Loss: 0.0580\n",
      "Epoch [50/250], Loss: 0.0578\n",
      "Checkpoint saved at epoch 50\n",
      "Epoch [51/250], Loss: 0.0575\n",
      "Epoch [52/250], Loss: 0.0574\n",
      "Epoch [53/250], Loss: 0.0571\n",
      "Epoch [54/250], Loss: 0.0569\n",
      "Epoch [55/250], Loss: 0.0567\n",
      "Epoch [56/250], Loss: 0.0565\n",
      "Epoch [57/250], Loss: 0.0562\n",
      "Epoch [58/250], Loss: 0.0560\n",
      "Epoch [59/250], Loss: 0.0558\n",
      "Epoch [60/250], Loss: 0.0556\n",
      "Epoch [61/250], Loss: 0.0554\n",
      "Epoch [62/250], Loss: 0.0552\n",
      "Epoch [63/250], Loss: 0.0550\n",
      "Epoch [64/250], Loss: 0.0548\n",
      "Epoch [65/250], Loss: 0.0545\n",
      "Epoch [66/250], Loss: 0.0544\n",
      "Epoch [67/250], Loss: 0.0541\n",
      "Epoch [68/250], Loss: 0.0539\n",
      "Epoch [69/250], Loss: 0.0538\n",
      "Epoch [70/250], Loss: 0.0535\n",
      "Epoch [71/250], Loss: 0.0533\n",
      "Epoch [72/250], Loss: 0.0531\n",
      "Epoch [73/250], Loss: 0.0530\n",
      "Epoch [74/250], Loss: 0.0527\n",
      "Epoch [75/250], Loss: 0.0525\n",
      "Epoch [76/250], Loss: 0.0524\n",
      "Epoch [77/250], Loss: 0.0521\n",
      "Epoch [78/250], Loss: 0.0520\n",
      "Epoch [79/250], Loss: 0.0518\n",
      "Epoch [80/250], Loss: 0.0516\n",
      "Epoch [81/250], Loss: 0.0515\n",
      "Epoch [82/250], Loss: 0.0512\n",
      "Epoch [83/250], Loss: 0.0511\n",
      "Epoch [84/250], Loss: 0.0509\n",
      "Epoch [85/250], Loss: 0.0507\n",
      "Epoch [86/250], Loss: 0.0506\n",
      "Epoch [87/250], Loss: 0.0504\n",
      "Epoch [88/250], Loss: 0.0502\n",
      "Epoch [89/250], Loss: 0.0500\n",
      "Epoch [90/250], Loss: 0.0498\n",
      "Epoch [91/250], Loss: 0.0497\n",
      "Epoch [92/250], Loss: 0.0495\n",
      "Epoch [93/250], Loss: 0.0494\n",
      "Epoch [94/250], Loss: 0.0492\n",
      "Epoch [95/250], Loss: 0.0491\n",
      "Epoch [96/250], Loss: 0.0490\n",
      "Epoch [97/250], Loss: 0.0488\n",
      "Epoch [98/250], Loss: 0.0487\n",
      "Epoch [99/250], Loss: 0.0485\n",
      "Epoch [100/250], Loss: 0.0484\n",
      "Checkpoint saved at epoch 100\n",
      "Epoch [101/250], Loss: 0.0482\n",
      "Epoch [102/250], Loss: 0.0482\n",
      "Epoch [103/250], Loss: 0.0480\n",
      "Epoch [104/250], Loss: 0.0478\n",
      "Epoch [105/250], Loss: 0.0477\n",
      "Epoch [106/250], Loss: 0.0476\n",
      "Epoch [107/250], Loss: 0.0475\n",
      "Epoch [108/250], Loss: 0.0474\n",
      "Epoch [109/250], Loss: 0.0473\n",
      "Epoch [110/250], Loss: 0.0472\n",
      "Epoch [111/250], Loss: 0.0470\n",
      "Epoch [112/250], Loss: 0.0469\n",
      "Epoch [113/250], Loss: 0.0468\n",
      "Epoch [114/250], Loss: 0.0467\n",
      "Epoch [115/250], Loss: 0.0467\n",
      "Epoch [116/250], Loss: 0.0465\n",
      "Epoch [117/250], Loss: 0.0465\n",
      "Epoch [118/250], Loss: 0.0463\n",
      "Epoch [119/250], Loss: 0.0462\n",
      "Epoch [120/250], Loss: 0.0461\n",
      "Epoch [121/250], Loss: 0.0460\n",
      "Epoch [122/250], Loss: 0.0460\n",
      "Epoch [123/250], Loss: 0.0458\n",
      "Epoch [124/250], Loss: 0.0457\n",
      "Epoch [125/250], Loss: 0.0458\n",
      "Epoch [126/250], Loss: 0.0456\n",
      "Epoch [127/250], Loss: 0.0455\n",
      "Epoch [128/250], Loss: 0.0454\n",
      "Epoch [129/250], Loss: 0.0454\n",
      "Epoch [130/250], Loss: 0.0453\n",
      "Epoch [131/250], Loss: 0.0451\n",
      "Epoch [132/250], Loss: 0.0451\n",
      "Epoch [133/250], Loss: 0.0450\n",
      "Epoch [134/250], Loss: 0.0450\n",
      "Epoch [135/250], Loss: 0.0449\n",
      "Epoch [136/250], Loss: 0.0449\n",
      "Epoch [137/250], Loss: 0.0447\n",
      "Epoch [138/250], Loss: 0.0448\n",
      "Epoch [139/250], Loss: 0.0446\n",
      "Epoch [140/250], Loss: 0.0445\n",
      "Epoch [141/250], Loss: 0.0445\n",
      "Epoch [142/250], Loss: 0.0445\n",
      "Epoch [143/250], Loss: 0.0443\n",
      "Epoch [144/250], Loss: 0.0443\n",
      "Epoch [145/250], Loss: 0.0442\n",
      "Epoch [146/250], Loss: 0.0442\n",
      "Epoch [147/250], Loss: 0.0440\n",
      "Epoch [148/250], Loss: 0.0441\n",
      "Epoch [149/250], Loss: 0.0441\n",
      "Epoch [150/250], Loss: 0.0440\n",
      "Checkpoint saved at epoch 150\n",
      "Epoch [151/250], Loss: 0.0439\n",
      "Epoch [152/250], Loss: 0.0439\n",
      "Epoch [153/250], Loss: 0.0438\n",
      "Epoch [154/250], Loss: 0.0438\n",
      "Epoch [155/250], Loss: 0.0437\n",
      "Epoch [156/250], Loss: 0.0437\n",
      "Epoch [157/250], Loss: 0.0437\n",
      "Epoch [158/250], Loss: 0.0435\n",
      "Epoch [159/250], Loss: 0.0435\n",
      "Epoch [160/250], Loss: 0.0435\n",
      "Epoch [161/250], Loss: 0.0434\n",
      "Epoch [162/250], Loss: 0.0434\n",
      "Epoch [163/250], Loss: 0.0434\n",
      "Epoch [164/250], Loss: 0.0432\n",
      "Epoch [165/250], Loss: 0.0433\n",
      "Epoch [166/250], Loss: 0.0433\n",
      "Epoch [167/250], Loss: 0.0433\n",
      "Epoch [168/250], Loss: 0.0431\n",
      "Epoch [169/250], Loss: 0.0430\n",
      "Epoch [170/250], Loss: 0.0430\n",
      "Epoch [171/250], Loss: 0.0430\n",
      "Epoch [172/250], Loss: 0.0429\n",
      "Epoch [173/250], Loss: 0.0430\n",
      "Epoch [174/250], Loss: 0.0429\n",
      "Epoch [175/250], Loss: 0.0428\n",
      "Epoch [176/250], Loss: 0.0428\n",
      "Epoch [177/250], Loss: 0.0429\n",
      "Epoch [178/250], Loss: 0.0426\n",
      "Epoch [179/250], Loss: 0.0427\n",
      "Epoch [180/250], Loss: 0.0425\n",
      "Epoch [181/250], Loss: 0.0427\n",
      "Epoch [182/250], Loss: 0.0427\n",
      "Epoch [183/250], Loss: 0.0425\n",
      "Epoch [184/250], Loss: 0.0424\n",
      "Epoch [185/250], Loss: 0.0422\n",
      "Epoch [186/250], Loss: 0.0424\n",
      "Epoch [187/250], Loss: 0.0424\n",
      "Epoch [188/250], Loss: 0.0424\n",
      "Epoch [189/250], Loss: 0.0423\n",
      "Epoch [190/250], Loss: 0.0423\n",
      "Epoch [191/250], Loss: 0.0423\n",
      "Epoch [192/250], Loss: 0.0422\n",
      "Epoch [193/250], Loss: 0.0422\n",
      "Epoch [194/250], Loss: 0.0423\n",
      "Epoch [195/250], Loss: 0.0422\n",
      "Epoch [196/250], Loss: 0.0421\n",
      "Epoch [197/250], Loss: 0.0420\n",
      "Epoch [198/250], Loss: 0.0421\n",
      "Epoch [199/250], Loss: 0.0420\n",
      "Epoch [200/250], Loss: 0.0422\n",
      "Checkpoint saved at epoch 200\n",
      "Epoch [201/250], Loss: 0.0421\n",
      "Epoch [202/250], Loss: 0.0419\n",
      "Epoch [203/250], Loss: 0.0419\n",
      "Epoch [204/250], Loss: 0.0420\n",
      "Epoch [205/250], Loss: 0.0418\n",
      "Epoch [206/250], Loss: 0.0419\n",
      "Epoch [207/250], Loss: 0.0417\n",
      "Epoch [208/250], Loss: 0.0419\n",
      "Epoch [209/250], Loss: 0.0418\n",
      "Epoch [210/250], Loss: 0.0417\n",
      "Epoch [211/250], Loss: 0.0417\n",
      "Epoch [212/250], Loss: 0.0418\n",
      "Epoch [213/250], Loss: 0.0416\n",
      "Epoch [214/250], Loss: 0.0419\n",
      "Epoch [215/250], Loss: 0.0415\n",
      "Epoch [216/250], Loss: 0.0417\n",
      "Epoch [217/250], Loss: 0.0418\n",
      "Epoch [218/250], Loss: 0.0417\n",
      "Epoch [219/250], Loss: 0.0414\n",
      "Epoch [220/250], Loss: 0.0414\n",
      "Epoch [221/250], Loss: 0.0416\n",
      "Epoch [222/250], Loss: 0.0415\n",
      "Epoch [223/250], Loss: 0.0415\n",
      "Epoch [224/250], Loss: 0.0414\n",
      "Epoch [225/250], Loss: 0.0414\n",
      "Epoch [226/250], Loss: 0.0413\n",
      "Epoch [227/250], Loss: 0.0413\n",
      "Epoch [228/250], Loss: 0.0415\n",
      "Epoch [229/250], Loss: 0.0413\n",
      "Epoch [230/250], Loss: 0.0411\n",
      "Epoch [231/250], Loss: 0.0413\n",
      "Epoch [232/250], Loss: 0.0412\n",
      "Epoch [233/250], Loss: 0.0412\n",
      "Epoch [234/250], Loss: 0.0413\n",
      "Epoch [235/250], Loss: 0.0412\n",
      "Epoch [236/250], Loss: 0.0412\n",
      "Epoch [237/250], Loss: 0.0412\n",
      "Epoch [238/250], Loss: 0.0412\n",
      "Epoch [239/250], Loss: 0.0411\n",
      "Epoch [240/250], Loss: 0.0411\n",
      "Epoch [241/250], Loss: 0.0411\n",
      "Epoch [242/250], Loss: 0.0411\n",
      "Epoch [243/250], Loss: 0.0411\n",
      "Epoch [244/250], Loss: 0.0410\n",
      "Epoch [245/250], Loss: 0.0409\n",
      "Epoch [246/250], Loss: 0.0411\n",
      "Epoch [247/250], Loss: 0.0411\n",
      "Epoch [248/250], Loss: 0.0410\n",
      "Epoch [249/250], Loss: 0.0410\n",
      "Epoch [250/250], Loss: 0.0408\n",
      "Checkpoint saved at epoch 250\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(sequences_tensor, targets_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model.to('cuda')\n",
    "checkpoint_dir = './checkpoints/'\n",
    "\n",
    "# Training loop\n",
    "loss_log = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_sequences, batch_targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_sequences)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate loss for the epoch\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    loss_log.append(avg_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint_path = f'{checkpoint_dir}model_epoch_{epoch+1}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Checkpoint saved at epoch {epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After 100 epochs, 0.0458 is the loss\n",
    "# After 250 epochs 0.0408 is the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss log saved to ./loss_log.txt\n"
     ]
    }
   ],
   "source": [
    "# loss_log[-5:]\n",
    "loss_log_file = './loss_log.txt'\n",
    "with open(loss_log_file, 'w') as f:\n",
    "    for epoch, loss in enumerate(loss_log):\n",
    "        f.write(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\\n')\n",
    "\n",
    "print(f'Loss log saved to {loss_log_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "C-qhBFqGgqBZ",
    "outputId": "2f86d928-ad59-493d-9491-ff02c9278b6b"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "\n",
    "# # Convert loss_log to a pandas Series or DataFrame for better handling\n",
    "# loss_series = pd.Series(loss_log)\n",
    "\n",
    "# # Plot using Seaborn\n",
    "# sns.lineplot(data=loss_series)\n",
    "# plt.title('Loss Over Iterations')\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoints/model_epoch_200.pth', weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tensor(string):\n",
    "    vectorised_start = []\n",
    "    for i in string:\n",
    "        vec = [0] * 35\n",
    "        vec[char_to_index[i]] = 1\n",
    "        vectorised_start.append(vec)\n",
    "    for i in range(20 - len(string)):\n",
    "        padding = [0] * 35\n",
    "        vectorised_start.insert(0, padding)\n",
    "    vectorised_start = np.array(vectorised_start)\n",
    "    start_encoded_tensor =  torch.tensor(vectorised_start, dtype=torch.float32)\n",
    "    start_encoded_tensor = start_encoded_tensor.unsqueeze(0)\n",
    "    return vectorised_start, start_encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "dWvJ-e_nOduo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "model.to('cpu')\n",
    "def get_n_next_char(tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "    probs = F.softmax(outputs, dim=-1)\n",
    "    predicted_index = torch.argmax(probs, dim=-1).item()\n",
    "    predicted_char = index_to_char[predicted_index]\n",
    "    print(f\"Predicted character: {predicted_char}\")\n",
    "    return predicted_index, predicted_char\n",
    "\n",
    "generated = \"\"\n",
    "vectorised_start, start_encoded_tensor = string_to_tensor('once upon a time a ')\n",
    "\n",
    "for i in range(1000):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(start_encoded_tensor)\n",
    "    probs = F.softmax(outputs, dim=-1)\n",
    "\n",
    "    # Get the index of the highest probability\n",
    "    predicted_index = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "    # Get the predicted character\n",
    "    predicted_char = index_to_char[predicted_index]\n",
    "\n",
    "    # print(f\"Predicted character: {predicted_char}\")\n",
    "    # print(\"First character of string: \",index_to_char[torch.argmax(start_encoded_tensor[0][0], dim=-1).item()])\n",
    "    generated += predicted_char\n",
    "\n",
    "    new_vec = [0] * 35\n",
    "    new_vec[predicted_index] = 1\n",
    "    vectorised_start = np.append(vectorised_start[1:], [new_vec], axis=0)\n",
    "\n",
    "    start_encoded_tensor =  torch.tensor(vectorised_start, dtype=torch.float32)\n",
    "    start_encoded_tensor = start_encoded_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"gon; and for used your never mulds.  sicinius: i wist be sarect and the greatneaters tenthers of mine to the gader a mull that would be consuly: you have beg as your hast, that would be menerian in rave the gods and be not as your pridentss of all the ever were here them all the people, the grangers on the market-place.  marcius: that in the former'd he hath smuld to love him so the bears and the carring.  menenius: the romer'd, when you are nay, which he was deserve to the people, my country of the wans to marcius, the people deserve the godst with me, sarm as my mother aufidius not but the gods; and that then? do not be not but my words of them.  all: which elainty, when it within thing turnels,' the reat with exirh they shall-- beliend him to such a still he did me deed i am me so now, where i think us the worthing the honour good their rome.  volumnia: i do beain the made him that had work he will be the stredpled: you shall be sarent he dishilous marcius, are the tribunes of the p\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_next_char(tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "    probs = F.softmax(outputs, dim=-1)\n",
    "    predicted_index = torch.argmax(probs, dim=-1).item()\n",
    "    predicted_char = index_to_char[predicted_index]\n",
    "    print(f\"Predicted character: {predicted_char}\")\n",
    "    return predicted_index, predicted_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iA42LkeaOZ5x",
    "outputId": "341edeb5-e866-431a-b939-9a2a93558bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted character: i\n",
      "(17, 'i')\n"
     ]
    }
   ],
   "source": [
    "_, x= string_to_tensor(\"hear\")\n",
    "y = get_next_char(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FOLLOWING ARE SMALL TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncwBHmeswdF3",
    "outputId": "bd697e3f-0e6a-4197-bbf9-fb5d704a649a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of start string: 20\n",
      "Vectorised start string shape: (20, 35)\n",
      "Tensor start string shape: torch.Size([1, 20, 35])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "start = \"once upon a time an \"\n",
    "print(\"Length of start string:\",len(start))\n",
    "vectorised_start = []\n",
    "for i in start:\n",
    "    vec = [0] * 35\n",
    "    vec[char_to_index[i]] = 1\n",
    "    vectorised_start.append(vec)\n",
    "for i in range(20 - len(start)):\n",
    "    padding = [0] * 35\n",
    "    vectorised_start.insert(0, padding)\n",
    "\n",
    "vectorised_start = np.array(vectorised_start)\n",
    "\n",
    "print(\"Vectorised start string shape:\",vectorised_start.shape)\n",
    "\n",
    "start_encoded_tensor =  torch.tensor(vectorised_start, dtype=torch.float32)\n",
    "start_encoded_tensor = start_encoded_tensor.unsqueeze(0)\n",
    "\n",
    "print(\"Tensor start string shape:\",start_encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "UwZHXtIcd64z",
    "outputId": "6dec4b6e-5715-4813-fd49-eaf33aaaca23"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the character from the tensor\n",
    "pos = -1\n",
    "index_to_char[torch.argmax(start_encoded_tensor[0][pos], dim=-1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JAUE_xuFlJI"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(start_encoded_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_OuC-YHF5S6",
    "outputId": "ff09b879-0d1f-4b2b-f759-811d1368ec96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8052, -2.2018, -2.3272, -2.0821, -1.4322, -1.8945, -1.9367, -2.2215,\n",
       "         -1.8308, -1.4844, -1.7142, -1.9555, -1.7564, -1.2421, -2.4002, -2.0735,\n",
       "         -2.2867, -1.5462, -1.8495, -2.0603, -1.5991, -2.1289, -1.7808, -0.9559,\n",
       "         -1.4678, -2.1032, -2.1087, -1.8054, -1.6473, -1.4099, -1.6015, -1.9589,\n",
       "         -1.9077, -1.5520, -2.0096]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjKlGbXNJFZb",
    "outputId": "f5584bd6-4e22-4697-8ab9-fc9d3bba3a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted character: o\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(outputs, dim=-1)\n",
    "\n",
    "# Get the index of the highest probability\n",
    "predicted_index = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "# Get the predicted character\n",
    "predicted_char = index_to_char[predicted_index]\n",
    "\n",
    "print(f\"Predicted character: {predicted_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZTWCXulMYH-"
   },
   "outputs": [],
   "source": [
    "new_vec = [0] * 35\n",
    "new_vec[predicted_index] = 1\n",
    "vectorised_start = np.append(vectorised_start[1:], [new_vec], axis=0)\n",
    "# To check if same shape\n",
    "print(\"Shape of vectorised start string\",vectorised_start.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4p_kbMH9JdiO",
    "outputId": "15ae4cbb-e7a1-452b-e7d3-2821904c1188",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted character:  \n",
      "First character of string:  o\n",
      "Predicted character:  \n",
      "First character of string:  n\n",
      "Predicted character:  \n",
      "First character of string:  c\n",
      "Predicted character:  \n",
      "First character of string:  e\n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:  u\n",
      "Predicted character:  \n",
      "First character of string:  p\n",
      "Predicted character:  \n",
      "First character of string:  o\n",
      "Predicted character:  \n",
      "First character of string:  n\n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:  a\n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:  t\n",
      "Predicted character:  \n",
      "First character of string:  i\n",
      "Predicted character:  \n",
      "First character of string:  m\n",
      "Predicted character:  \n",
      "First character of string:  e\n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:  a\n",
      "Predicted character:  \n",
      "First character of string:  n\n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n",
      "Predicted character:  \n",
      "First character of string:   \n"
     ]
    }
   ],
   "source": [
    "generated = \"\"\n",
    "for i in range(30):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(start_encoded_tensor)\n",
    "    probs = F.softmax(outputs, dim=-1)\n",
    "\n",
    "    # Get the index of the highest probability\n",
    "    predicted_index = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "    # Get the predicted character\n",
    "    predicted_char = index_to_char[predicted_index]\n",
    "\n",
    "    print(f\"Predicted character: {predicted_char}\")\n",
    "    print(\"First character of string: \",index_to_char[torch.argmax(start_encoded_tensor[0][0], dim=-1).item()])\n",
    "    generated += predicted_char\n",
    "\n",
    "    new_vec = [0] * 35\n",
    "    new_vec[predicted_index] = 1\n",
    "    vectorised_start = np.append(vectorised_start[1:], [new_vec], axis=0)\n",
    "\n",
    "    start_encoded_tensor =  torch.tensor(vectorised_start, dtype=torch.float32)\n",
    "    start_encoded_tensor = start_encoded_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "_wWpPzFWK9HK"
   },
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "torch.save(model.state_dict(), 'shakespeare_loss_0.0461.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEEx7sjIL_95"
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# LOAD MODEL PARAMETERS\n",
    "model.load_state_dict(torch.load('rnnmodel.pth'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO+MMoq8fryX+BxYyrF1RgL",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
